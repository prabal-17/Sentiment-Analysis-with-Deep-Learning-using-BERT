{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9534320,"sourceType":"datasetVersion","datasetId":5806871}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/prabalpratapsinghml/sentiment-analysis-with-deep-learning-bert-ipynb?scriptVersionId=199246885\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Sentiment Analysis with Deep Learning using BERT","metadata":{}},{"cell_type":"markdown","source":"### Prerequisites","metadata":{}},{"cell_type":"markdown","source":"- Intermediate-level knowledge of Python 3 (NumPy and Pandas preferably, but not required)\n- Exposure to PyTorch usage\n- Basic understanding of Deep Learning and Language Models (BERT specifically)","metadata":{}},{"cell_type":"markdown","source":"### Project Outline","metadata":{}},{"cell_type":"markdown","source":"**Task 1**: Introduction (this section)\n\n**Task 2**: Exploratory Data Analysis and Preprocessing\n\n**Task 3**: Training/Validation Split\n\n**Task 4**: Loading Tokenizer and Encoding our Data\n\n**Task 5**: Setting up BERT Pretrained Model\n\n**Task 6**: Creating Data Loaders\n\n**Task 7**: Setting Up Optimizer and Scheduler\n\n**Task 8**: Defining our Performance Metrics\n\n**Task 9**: Creating our Training Loop\n\n**Task 10**: Loading and Evaluating our Model","metadata":{}},{"cell_type":"markdown","source":"## Task 1: Introduction","metadata":{}},{"cell_type":"markdown","source":"### What is BERT\n\nBERT is a large-scale transformer-based Language Model that can be finetuned for a variety of tasks.\n\nFor more information, the original paper can be found [here](https://arxiv.org/abs/1810.04805). \n\n[HuggingFace documentation](https://huggingface.co/transformers/model_doc/bert.html)\n\n[Bert documentation](https://characters.fandom.com/wiki/Bert_(Sesame_Street) ;)","metadata":{}},{"cell_type":"markdown","source":"![https://images.app.goo.gl/ySVq9XEsd9227gFW7](http://)","metadata":{}},{"cell_type":"markdown","source":"## Task 2: Exploratory Data Analysis and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We will use the SMILE Twitter dataset.\n\n_Wang, Bo; Tsakalidis, Adam; Liakata, Maria; Zubiaga, Arkaitz; Procter, Rob; Jensen, Eric (2016): SMILE Twitter Emotion dataset. figshare. Dataset. https://doi.org/10.6084/m9.figshare.3187909.v2_","metadata":{}},{"cell_type":"code","source":"# !pip install ipywidgets\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.283406Z","iopub.execute_input":"2024-10-02T20:10:52.283838Z","iopub.status.idle":"2024-10-02T20:10:52.289084Z","shell.execute_reply.started":"2024-10-02T20:10:52.2838Z","shell.execute_reply":"2024-10-02T20:10:52.287849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/smile-is-funded-by-the-ahrc-www-ahrc-ac-uk/smileannotationsfinal.csv',names=['id','text','category'])\n\ndf.set_index('id',inplace=True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.291142Z","iopub.execute_input":"2024-10-02T20:10:52.291494Z","iopub.status.idle":"2024-10-02T20:10:52.331591Z","shell.execute_reply.started":"2024-10-02T20:10:52.291457Z","shell.execute_reply":"2024-10-02T20:10:52.33024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.334123Z","iopub.execute_input":"2024-10-02T20:10:52.334948Z","iopub.status.idle":"2024-10-02T20:10:52.350595Z","shell.execute_reply.started":"2024-10-02T20:10:52.334888Z","shell.execute_reply":"2024-10-02T20:10:52.349621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.category.value_counts()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.351777Z","iopub.execute_input":"2024-10-02T20:10:52.352129Z","iopub.status.idle":"2024-10-02T20:10:52.366808Z","shell.execute_reply.started":"2024-10-02T20:10:52.352086Z","shell.execute_reply":"2024-10-02T20:10:52.365884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[~df.category.str.contains('\\|')]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.369361Z","iopub.execute_input":"2024-10-02T20:10:52.369733Z","iopub.status.idle":"2024-10-02T20:10:52.379192Z","shell.execute_reply.started":"2024-10-02T20:10:52.369692Z","shell.execute_reply":"2024-10-02T20:10:52.378157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df.category != 'nocode']","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.380847Z","iopub.execute_input":"2024-10-02T20:10:52.381488Z","iopub.status.idle":"2024-10-02T20:10:52.390073Z","shell.execute_reply.started":"2024-10-02T20:10:52.381434Z","shell.execute_reply":"2024-10-02T20:10:52.388929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.category.value_counts()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.391293Z","iopub.execute_input":"2024-10-02T20:10:52.391674Z","iopub.status.idle":"2024-10-02T20:10:52.402853Z","shell.execute_reply.started":"2024-10-02T20:10:52.391633Z","shell.execute_reply":"2024-10-02T20:10:52.401814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"possible_labels = df.category.unique()\nprint(possible_labels)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.404234Z","iopub.execute_input":"2024-10-02T20:10:52.404657Z","iopub.status.idle":"2024-10-02T20:10:52.412188Z","shell.execute_reply.started":"2024-10-02T20:10:52.404604Z","shell.execute_reply":"2024-10-02T20:10:52.411044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dic ={}\nfor index,possible_label in enumerate(possible_labels):\n    label_dic[possible_label] =index","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.41356Z","iopub.execute_input":"2024-10-02T20:10:52.413958Z","iopub.status.idle":"2024-10-02T20:10:52.419605Z","shell.execute_reply.started":"2024-10-02T20:10:52.41391Z","shell.execute_reply":"2024-10-02T20:10:52.418557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dic","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.420781Z","iopub.execute_input":"2024-10-02T20:10:52.421149Z","iopub.status.idle":"2024-10-02T20:10:52.432197Z","shell.execute_reply.started":"2024-10-02T20:10:52.421112Z","shell.execute_reply":"2024-10-02T20:10:52.431119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'] = df.category.replace(label_dic)\ndf.head()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.441063Z","iopub.execute_input":"2024-10-02T20:10:52.441381Z","iopub.status.idle":"2024-10-02T20:10:52.459377Z","shell.execute_reply.started":"2024-10-02T20:10:52.441347Z","shell.execute_reply":"2024-10-02T20:10:52.45821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 3: Training/Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.461986Z","iopub.execute_input":"2024-10-02T20:10:52.462681Z","iopub.status.idle":"2024-10-02T20:10:52.467336Z","shell.execute_reply.started":"2024-10-02T20:10:52.46263Z","shell.execute_reply":"2024-10-02T20:10:52.466337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_val,y_train,y_val = train_test_split(df.index.values,df.label.values,test_size=0.15,random_state=17,stratify=df.label.values)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.468526Z","iopub.execute_input":"2024-10-02T20:10:52.468879Z","iopub.status.idle":"2024-10-02T20:10:52.480427Z","shell.execute_reply.started":"2024-10-02T20:10:52.468821Z","shell.execute_reply":"2024-10-02T20:10:52.479252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['data_type']=['not_set']*df.shape[0]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.482071Z","iopub.execute_input":"2024-10-02T20:10:52.48276Z","iopub.status.idle":"2024-10-02T20:10:52.489072Z","shell.execute_reply.started":"2024-10-02T20:10:52.48271Z","shell.execute_reply":"2024-10-02T20:10:52.487787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.49263Z","iopub.execute_input":"2024-10-02T20:10:52.493048Z","iopub.status.idle":"2024-10-02T20:10:52.506058Z","shell.execute_reply.started":"2024-10-02T20:10:52.49301Z","shell.execute_reply":"2024-10-02T20:10:52.504778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[x_train,'data_type']='train'\ndf.loc[x_val,'data_type']='val'","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.507523Z","iopub.execute_input":"2024-10-02T20:10:52.507932Z","iopub.status.idle":"2024-10-02T20:10:52.515804Z","shell.execute_reply.started":"2024-10-02T20:10:52.50785Z","shell.execute_reply":"2024-10-02T20:10:52.514828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['category','label','data_type']).count()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T20:10:52.520222Z","iopub.execute_input":"2024-10-02T20:10:52.520536Z","iopub.status.idle":"2024-10-02T20:10:52.539096Z","shell.execute_reply.started":"2024-10-02T20:10:52.520502Z","shell.execute_reply":"2024-10-02T20:10:52.537942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 4: Loading Tokenizer and Encoding our Data","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.541115Z","iopub.execute_input":"2024-10-02T20:10:52.541578Z","iopub.status.idle":"2024-10-02T20:10:52.54666Z","shell.execute_reply.started":"2024-10-02T20:10:52.541524Z","shell.execute_reply":"2024-10-02T20:10:52.545558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased'\n                                         ,do_lower_case =True\n                                         )","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:52.548033Z","iopub.execute_input":"2024-10-02T20:10:52.548387Z","iopub.status.idle":"2024-10-02T20:10:53.628224Z","shell.execute_reply.started":"2024-10-02T20:10:52.548335Z","shell.execute_reply":"2024-10-02T20:10:53.626689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_train = tokenizer.batch_encode_plus(\n    df[df.data_type=='train'].text.values,\n    add_special_tokens=True,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\nencoded_data_val = tokenizer.batch_encode_plus(\n    df[df.data_type=='val'].text.values,\n    add_special_tokens=True,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\ninput_ids_train =encoded_data_train['input_ids']\nattention_masks_train=encoded_data_train['attention_mask']\nlabels_train=torch.tensor(df[df.data_type=='train'].label.values)\ninput_ids_val =encoded_data_val['input_ids']\nattention_masks_val=encoded_data_val['attention_mask']\nlabels_val=torch.tensor(df[df.data_type=='val'].label.values)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:53.630172Z","iopub.execute_input":"2024-10-02T20:10:53.630949Z","iopub.status.idle":"2024-10-02T20:10:55.713335Z","shell.execute_reply.started":"2024-10-02T20:10:53.630897Z","shell.execute_reply":"2024-10-02T20:10:55.712365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train,attention_masks_train,labels_train)\ndataset_val = TensorDataset(input_ids_val,attention_masks_val,labels_val)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:55.715238Z","iopub.execute_input":"2024-10-02T20:10:55.715717Z","iopub.status.idle":"2024-10-02T20:10:55.721813Z","shell.execute_reply.started":"2024-10-02T20:10:55.715673Z","shell.execute_reply":"2024-10-02T20:10:55.72067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset_train)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:55.722822Z","iopub.execute_input":"2024-10-02T20:10:55.723177Z","iopub.status.idle":"2024-10-02T20:10:55.732275Z","shell.execute_reply.started":"2024-10-02T20:10:55.723139Z","shell.execute_reply":"2024-10-02T20:10:55.731248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset_val)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:55.733513Z","iopub.execute_input":"2024-10-02T20:10:55.733854Z","iopub.status.idle":"2024-10-02T20:10:55.741361Z","shell.execute_reply.started":"2024-10-02T20:10:55.733817Z","shell.execute_reply":"2024-10-02T20:10:55.740314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 5: Setting up BERT Pretrained Model","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:55.746008Z","iopub.execute_input":"2024-10-02T20:10:55.747031Z","iopub.status.idle":"2024-10-02T20:10:55.75138Z","shell.execute_reply.started":"2024-10-02T20:10:55.746991Z","shell.execute_reply":"2024-10-02T20:10:55.750288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=len(label_dic),output_attentions=False,\n                                     output_hidden_states=False\n                                     )","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:10:55.752666Z","iopub.execute_input":"2024-10-02T20:10:55.753027Z","iopub.status.idle":"2024-10-02T20:11:02.595125Z","shell.execute_reply.started":"2024-10-02T20:10:55.75299Z","shell.execute_reply":"2024-10-02T20:11:02.594228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 6: Creating Data Loaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:02.596488Z","iopub.execute_input":"2024-10-02T20:11:02.596952Z","iopub.status.idle":"2024-10-02T20:11:02.602612Z","shell.execute_reply.started":"2024-10-02T20:11:02.596899Z","shell.execute_reply":"2024-10-02T20:11:02.601336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataloader = DataLoader(\n    dataset_train,\n    sampler=RandomSampler(dataset_train),  # Shuffle the data for training\n    batch_size=4  # You can set this to the batch size you prefer\n)\n\n# Create DataLoader for validation\nval_dataloader = DataLoader(\n    dataset_val,\n    sampler=SequentialSampler(dataset_val),  # No shuffling for validation\n    batch_size=32  # Same batch size or you can adjust for validation\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:02.604076Z","iopub.execute_input":"2024-10-02T20:11:02.604494Z","iopub.status.idle":"2024-10-02T20:11:02.612417Z","shell.execute_reply.started":"2024-10-02T20:11:02.60444Z","shell.execute_reply":"2024-10-02T20:11:02.611444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 7: Setting Up Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:02.613638Z","iopub.execute_input":"2024-10-02T20:11:02.614043Z","iopub.status.idle":"2024-10-02T20:11:02.620096Z","shell.execute_reply.started":"2024-10-02T20:11:02.613989Z","shell.execute_reply":"2024-10-02T20:11:02.619035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the learning rate\nlearning_rate = 1e-5\n\n# Define the optimizer for the BERT model (only for the model parameters, not other variables)\noptimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:02.621469Z","iopub.execute_input":"2024-10-02T20:11:02.621913Z","iopub.status.idle":"2024-10-02T20:11:03.15208Z","shell.execute_reply.started":"2024-10-02T20:11:02.621844Z","shell.execute_reply":"2024-10-02T20:11:03.151019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set total number of training epochs\nepochs = 10\n\n# Calculate the total number of training steps\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler with a warmup phase\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0,  # Number of warmup steps (you can adjust this value)\n    num_training_steps=total_steps  # Total number of training steps\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:03.153103Z","iopub.execute_input":"2024-10-02T20:11:03.153599Z","iopub.status.idle":"2024-10-02T20:11:03.160659Z","shell.execute_reply.started":"2024-10-02T20:11:03.153563Z","shell.execute_reply":"2024-10-02T20:11:03.158483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 8: Defining our Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"Accuracy metric approach originally used in accuracy function in [this tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#41-bertforsequenceclassification).","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:03.162578Z","iopub.execute_input":"2024-10-02T20:11:03.163454Z","iopub.status.idle":"2024-10-02T20:11:03.232207Z","shell.execute_reply.started":"2024-10-02T20:11:03.163396Z","shell.execute_reply":"2024-10-02T20:11:03.230958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:03.233825Z","iopub.execute_input":"2024-10-02T20:11:03.234309Z","iopub.status.idle":"2024-10-02T20:11:03.24064Z","shell.execute_reply.started":"2024-10-02T20:11:03.234268Z","shell.execute_reply":"2024-10-02T20:11:03.239533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    # Calculate the F1 score (macro-average)\n    return f1_score(labels_flat, preds_flat, average='weighted')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:03.242131Z","iopub.execute_input":"2024-10-02T20:11:03.242519Z","iopub.status.idle":"2024-10-02T20:11:03.250354Z","shell.execute_reply.started":"2024-10-02T20:11:03.242474Z","shell.execute_reply":"2024-10-02T20:11:03.248993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_per_class(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    # Get the unique labels\n    unique_labels = np.unique(labels_flat)\n    \n    for label in unique_labels:\n        # Find the indexes where the true label matches the current class\n        label_idx = np.where(labels_flat == label)\n        \n        # Calculate the number of correct predictions for the current class\n        correct_preds = np.sum(preds_flat[label_idx] == labels_flat[label_idx])\n        \n        # Calculate the accuracy for the current class\n        accuracy = correct_preds / len(label_idx[0])\n        \n        print(f\"Accuracy for label {label}: {accuracy:.2f}\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:03.251474Z","iopub.execute_input":"2024-10-02T20:11:03.251829Z","iopub.status.idle":"2024-10-02T20:11:03.260658Z","shell.execute_reply.started":"2024-10-02T20:11:03.251793Z","shell.execute_reply":"2024-10-02T20:11:03.25944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 9: Creating our Training Loop","metadata":{}},{"cell_type":"markdown","source":"Approach adapted from an older version of HuggingFace's `run_glue.py` script. Accessible [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128).","metadata":{}},{"cell_type":"code","source":"import random\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:11:03.261736Z","iopub.execute_input":"2024-10-02T20:11:03.262063Z","iopub.status.idle":"2024-10-02T20:11:03.27718Z","shell.execute_reply.started":"2024-10-02T20:11:03.262027Z","shell.execute_reply":"2024-10-02T20:11:03.27619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if a GPU is available and if not, fallback to CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Transfer the model to the selected device\nmodel.to(device)\n\n# Print the device being used\nprint(f'Model is using device: {device}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:28:51.564448Z","iopub.execute_input":"2024-10-02T20:28:51.564927Z","iopub.status.idle":"2024-10-02T20:28:51.738024Z","shell.execute_reply.started":"2024-10-02T20:28:51.564855Z","shell.execute_reply":"2024-10-02T20:28:51.736944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:31:55.139723Z","iopub.execute_input":"2024-10-02T20:31:55.140173Z","iopub.status.idle":"2024-10-02T20:31:55.150132Z","shell.execute_reply.started":"2024-10-02T20:31:55.14013Z","shell.execute_reply":"2024-10-02T20:31:55.149031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# Training Loop\nfor epoch in range(1, epochs + 1):\n    model.train()\n    \n    loss_train_total = 0\n    \n    # Create a single progress bar for the entire epoch\n    progress_bar = tqdm(total=len(train_dataloader), desc=f'Epoch {epoch}', leave=False)\n\n    for batch in train_dataloader:\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {\n            'input_ids': batch[0],\n            'attention_mask': batch[1],\n            'labels': batch[2],\n        }\n        \n        outputs = model(**inputs)\n        loss = outputs[0]\n        loss_train_total += loss.item()\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        # Update progress bar\n        progress_bar.update(1)  # Increment progress bar by 1 for each batch\n\n    # Close the progress bar at the end of the epoch\n    progress_bar.close()\n    \n    # Save the model after each epoch\n    torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')\n    \n    loss_train_avg = loss_train_total / len(train_dataloader)\n    print(f'Training loss: {loss_train_avg:.3f}')\n    \n    # Evaluate on validation set\n    val_loss, predictions, true_vals = evaluate(val_dataloader)\n    val_f1 = f1_score_func(predictions, true_vals)\n    \n    # Display validation results\n    print(f'Validation loss: {val_loss}')\n    print(f'F1 Score (Weighted): {val_f1}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:13:53.443618Z","iopub.execute_input":"2024-10-02T20:13:53.44442Z","iopub.status.idle":"2024-10-02T20:24:50.167282Z","shell.execute_reply.started":"2024-10-02T20:13:53.444375Z","shell.execute_reply":"2024-10-02T20:24:50.166065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 10: Loading and Evaluating our Model","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dic),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-10-02T20:26:35.596435Z","iopub.execute_input":"2024-10-02T20:26:35.596903Z","iopub.status.idle":"2024-10-02T20:26:35.912785Z","shell.execute_reply.started":"2024-10-02T20:26:35.596846Z","shell.execute_reply":"2024-10-02T20:26:35.911638Z"},"trusted":true},"execution_count":null,"outputs":[]}]}